---
title: "Framingham Heart Study Analysis"
author: "HP"
date: "2024-03-24"
output: html_document
---
### Loading packages

```{r warning=FALSE, include=FALSE}
library(car)
library(corrplot)
library(leaps)
library(pls)
library(DataExplorer)
library(tidyr)
library(ggplot2)
library(MASS)
library(nnet)
library(e1071)
library(class)
library(randomcoloR)
library(gridExtra)
library(caret)
library(regclass)

library(scales)
library(caret)
library(randomForest)
library(xgboost)
library(e1071)
library(magrittr)
library(partykit)
library(dplyr)
library(magrittr)
library(naivebayes)
library(party)
library(MLmetrics)
library(pROC)
library(mlr)
library(Metrics)
library(knitr)
library(xtable)
library(xgboost)

set.seed(31)
```

### Data Loading and exploration

```{r}
# loading dataset
data <- read.csv('framingham.csv')
head(data)
summary(data)



# Checking for missing data and imputing with mean of the column where missing
colSums(is.na(data))
data.frame(colSums(is.na(data)))
dim(data[rowSums(is.na(data))!=0,])
data2 <- apply(data, 2, function(x) {
  ifelse(is.na(x), mean(x, na.rm = TRUE), x)
})
data2 <- as.data.frame(data2)
colSums(is.na(data2))
data2$TenYearCHD = as.factor(data2$TenYearCHD)

```
```{r}
dim(data)
xtable(data.frame(sapply(data,class)))
```

### Testing Training

```{r}
# Split Data into Training and Testing in R 
percent = 0.8
sample_size = floor(percent*nrow(data2))

# randomly split data in r
picked = sample(seq_len(nrow(data2)),size = sample_size)
data2_train =data2[picked,]
data2_test =data2[-picked,]
ytrain=data2_train$TenYearCHD
ytest=data2_test$TenYearCHD
```

##EDA

```{r EDA}
## Corrplot
acorr <- cor(data2_train[,-16])
corrplot::corrplot(acorr, method= 'circle')

## histogram
hist_plot <- function(i){
  ggplot(data2_train, aes(x = data2_train[,i])) +
    geom_histogram(color = "black", fill = randomColor()) + xlab(names(data2_train[i]))
}
## Return hist
do.call(grid.arrange, lapply(c(2,3,5,10,11,12,13,14,15), hist_plot))
```


```{r}
fdata2 <-sapply(data2_train, as.factor)[,0:9]
fresponse <- as.factor(data2_train$TenYearCHD)
fdata2<- data2_test[,0:9]

```


```{r}
box_plot <- function(i){
  ggplot(fdata2, aes(x = fresponse, y = fdata2[,i])) +
    geom_boxplot(color = "black", fill = randomColor()) + xlab('TenYearCHD') + ylab(names(fdata2[i]))
}

## Return boxplots
do.call(grid.arrange, lapply(1:9, box_plot))

```


```{r Bar plot}
# # Bar plot code
age = xtabs(~data$TenYearCHD+data$age)
gender = xtabs(~data$TenYearCHD+data$male)
colnames(gender) = c("Female", "Male")
edu = xtabs(~data$TenYearCHD+data$education)
smoker = xtabs(~data$TenYearCHD+data$currentSmoker)
colnames(smoker) = c("Non-Smoker", "Smoker")
barplot(prop.table(age), axes=T, space=0.3, horiz=T,
xlab="Proportion of No Heart Disease (darkgreen) vs Heart Disease (lightgreen)",
col=c("darkgreen","lightgreen"), main="Heart Disease by Age Group")
barplot(prop.table(gender), axes=T, space=0.3, horiz=T,
xlab="Proportion of No Heart Disease (darkgreen) vs Heart Disease (lightgreen)",
col=c("darkgreen","lightgreen"), main="Heart Disease by Gender")
barplot(prop.table(edu), axes=T, space=0.3, horiz=T,
xlab="Proportion of No Heart Disease (darkgreen) vs Heart Disease (lightgreen)",
col=c("darkgreen","lightgreen"), main="Heart Disease by Education Level")
barplot(prop.table(smoker), axes=T, space=0.3, horiz=T,
xlab="Proportion of No Heart Disease (darkgreen) vs Heart Disease (lightgreen)",
col=c("darkgreen","lightgreen"), main="Heart Disease by Smoker")
```



### Tree
Grow Tree

```{r}
## grow tree
rpart.chd <- rpart(TenYearCHD ~ .,data=data2_train, method="class", parms=list(split="gini"))

## Initial Tree T0, we have 5 terminal nodes
print(rpart.chd)
post(rpart.chd,filename="")

## Or simplified plot
plot(rpart.chd,compress=TRUE)
text(rpart.chd)
```
### Testing/Training Errors for T0

```{r Testing/Training}
test_seq= seq(from = .3, to=.7, by= 0.05)
opt_frame = data.frame()

for (i in test_seq){
  ## Training & Test Errors for Tree T0
  y1hatc <- ifelse(predict(rpart.chd,data2_train)[,2] < i, 0, 1)
  test_error = sum(y1hatc != ytrain)/length(ytrain)
  #  0.1412979 (training error for T0)
  y2hatc <-  predict(rpart.chd, data2_test[,-16],type="class")
  train_error = sum(y2hatc != ytest)/length(ytest)
  # 0.1603774 (test error for T0)
  opt_frame = rbind(opt_frame, data.frame('split' = i, "training error" = train_error, "testing error" = test_error))
}

xtable(opt_frame, digits=6)
```


## To determine whether the tree T0 is appropriate or if some of 
##    the branches need to be pruned 

```{r}
plotcp(rpart.chd)

printcp(rpart.chd)
```


## or 
print(rpart.chd$cptable)

## The xerror column is the estimates of cross-validated prediction
##   error for different numbers of splits. 
##   Here the best tree turns out to be T0

```{r}
opt <- which.min(rpart.chd$cptable[, "xerror"]); 
cp1 <- rpart.chd$cptable[opt, "CP"];
rpart.pruned1 <- prune(rpart.chd,cp=cp1);
## This is same as T0 in this example. Maybe different in other problems
```


```{r}
## Try another cp
#cp1 <- 0.0262;
rpart.pruned1 <- prune(rpart.chd,cp=cp1);
y2hatc1 <-  predict(rpart.pruned1, data2_test[,-16],type="class")
sum(y2hatc1 != ytest)/length(ytest)
## 0.1391509 (test error for the pruned tree, vs 0.1035156 for T0)
```



## Compare T01 and T1, collapes V57

```{r}
plotcp(rpart.chd)
cp2 <- 0.03; 
rpart.pruned2 <- prune(rpart.chd,cp=cp2)
```



## Random Forest
```{r}
## Build Random Forest with the default parameters
## It can be 'classification', 'regression', or 'unsupervised'

chd_rf1 <- randomForest(as.factor(data2_train$TenYearCHD) ~., data=data2_train, 
                    importance=TRUE)
```

```{r}
## Check Important variables
importance(chd_rf1)
## There are two types of importance measure 
##  (1=mean decrease in accuracy, 
##   2= mean decrease in node impurity)
importance(chd_rf1, type=2)
varImpPlot(chd_rf1)

## The plots show that sysBP, age, diaBP, BMI are among the most 
##     important features when predicting V58. 



```
## Prediction Error
```{r}

## Prediction on the training data set
rf_pred_tr = predict(chd_rf1, data2_train, type='class')
table(rf_pred_tr, data2_train$TenYearCHD)
rf_train_error = sum(rf_pred_tr != ytrain)/length(ytrain)

## Prediction on the testing data set
rf_pred_te = predict(chd_rf1, data2_test, type='class')
table(rf_pred_te, ytest)
rf_test_error = sum(rf_pred_te != ytest)/length(ytest)

xtable(data.frame("training error" = rf_train_error,"test error" = rf_test_error), digits=6)

```
## Tuning Random Forest

```{r}

##In practice, You can fine-tune parameters in Random Forest such as 
#ntree = number of tress to grow, and the default is 500. 
#mtry = number of variables randomly sampled as candidates at each split. 
#          The default is sqrt(p) for classfication and p/3 for regression
#nodesize = minimum size of terminal nodes. 
#           The default value is 1 for classification and 5 for regression

ntree_list = seq(300,700,50)
mtry_list = seq(1,10,1)
node_list = seq(1,6,1)

loop_df = data.frame()

for (nt in ntree_list){
  for (m in mtry_list){
    for (n in node_list){
      
      chd_rf2 <- randomForest(as.factor(data2_train$TenYearCHD) ~., data=data2_train, 
                    importance=TRUE, ntree = nt, mtry = m, nodesize=n)
      ## Prediction on the testing data set
      rf_pred_tr2 = predict(chd_rf2, data2_train, type='class')
      rf_train_error2 = sum(rf_pred_tr2 != ytrain)/length(ytrain)
      
      ## Prediction on the testing data set
      rf_pred_te2 = predict(chd_rf1, data2_test, type='class')
      rf_test_error2 = sum(rf_pred_te2 != ytest)/length(ytest)
      
      holddf = data.frame("ntree" = nt, "mtry" = m, "nodesize" = n, "training error" = rf_train_error2, "test error" = rf_test_error2)
      
      loop_df = rbind(loop_df, holddf)
    }
  }
}

loop_df

```

```{r}
top_choices = loop_df[loop_df['training.error']>.04,]
top_choices = top_choices[order(top_choices$test.error),]
xtable(head(top_choices, 15), digits =6)
```

### XGBoost

```{r}
# XGBoost
xgb_train <- xgb.DMatrix(data = as.matrix(data2_train[,-16]), label = ytrain)
xgb_test <- xgb.DMatrix(data = as.matrix(data2_test[,-16]), label = ytest)

xgb_params <- list(
  booster = "gbtree",
  eta = 0.01,
  max_depth = 8,
  gamma = 4,
  subsample = 0.75,
  colsample_bytree = 1,
  objective = "multi:softprob",
  eval_metric = "mlogloss",
  num_class = length(levels(data2_train$TenYearCHD))+1
)

xgb_model <- xgb.train(
  params = xgb_params,
  data = xgb_train,
  nrounds = 5000,
  verbose = 1
)



```

```{r}
## Training Error
xgb_preds_tr <- predict(xgb_model, as.matrix(data2_train[,-16]), reshape=TRUE, type='class')>.5
xgb_tr = sum(as.numeric(xgb_preds_tr[,1]) != ytrain)/length(ytrain)

## Testing Error
xgb_preds_te = predict(xgb_model, as.matrix(data2_test[,-16]), reshape = TRUE, type='class')>.5
xgb_te = sum(as.numeric(xgb_preds_te[,1]) != ytest)/length(ytest)

print(xgb_tr)
print(xgb_te)
```



## Logistic regression

```{r}
# Ensure the outcome is a factor
data2$TenYearCHD <- as.factor(data2$TenYearCHD)

modlogistic <- glm( TenYearCHD ~ ., family = binomial(link = "logit"), data= data2_train);
phatlogistic  <-  predict(modlogistic, data2_test[,-16],type="response")
yhatlogistic <- ifelse(phatlogistic  <0.5,0,1)
sum(yhatlogistic  != ytest)/length(ytest)
##0.1438679

```

## MONTE CARLO

```{r}

### 4. Monte_carlo
### The following R code might be useful, but you need to modify it.
## fat is full data set
n1 = dim(data2_train)[1]
n = dim(data2)[1]; ## the total sample size
set.seed(31); ### you can also set othernumber for randomization seed if you want
### Initialize the TE values for all models in all $B=100$ loops
B <- 50; ### number of loopsx
TEALL <- matrix(NA, nrow = B, ncol = 14)

for (b in 1:B){
### randomly select n1 observations as a new training subset in each loop

  flag <- sort(sample(1:n, n1))
  data2_train_temp <- data2[flag, ] # Temp training set for CV
  data2_test_temp <- data2[-flag, ] # Temp testing set for CV
  
  
  ## Method 1.1: LDA - ALL
  mod1 <- lda(data2_train_temp[,-16], data2_train_temp[,16]); 
  
  pred1train <- predict(mod1,data2_train_temp[,-16])$class; 
  tr_lda <- mean(pred1train != data2_train_temp$TenYearCHD)
  
  pred1test <- predict(mod1,data2_test_temp[,-16])$class; 
  te_lda <- mean(pred1test != data2_test_temp$TenYearCHD)

  ## Method 2.1: QDA
  mod2 <- qda(data2_train_temp[,-16], data2_train_temp[,16])
  tr_qda <-  mean( predict(mod2,data2_train_temp[,-16])$class != data2_train_temp$TenYearCHD)
  te_qda <-  mean( predict(mod2,data2_test_temp[,-16])$class != data2_test_temp$TenYearCHD)

  
  ## Method 3.1: Naive Bayes
  mod3 <- naiveBayes( data2_train_temp[,-16], data2_train_temp[,16])
  tr_bayes <- mean( predict(mod3,data2_train_temp[,-16]) != data2_train_temp$TenYearCHD)
  te_bayes <- mean( predict(mod3,data2_test_temp[,-16]) != data2_test_temp$TenYearCHD)

  
  ## Method 4.1: (multinomial) logisitic regression) 
  mod4 <- multinom( TenYearCHD ~ ., family = binomial, data=data2_train_temp, trace = FALSE) 
  tr_logistic <- mean( predict(mod4,data2_train_temp[,-16]) != data2_train_temp$TenYearCHD)
  te_logistic <- mean( predict(mod4,data2_test_temp[,-16]) != data2_test_temp$TenYearCHD)


  ## Method 5.1: KNN
  #Find best K using cross validation on the training data based on full model
  trControl = caret::trainControl(method  = "cv", number  = 10)
  knn_model = caret::train(as.factor(TenYearCHD) ~ ., method     = "knn",tuneGrid   = expand.grid(k = 1:20),
                    trControl = trControl,metric= "Accuracy",data=data2_train_temp)
  kk = knn_model$bestTune$k
  k_whole <- kk

  ypred2.train <- knn(data2_train_temp[,-16], data2_train_temp[,-16], data2_train_temp[,16], k=kk);
  tr_knn <- mean( ypred2.train != data2_train_temp[,16])

  ypred2.test <- knn(data2_train_temp[,-16], data2_test_temp[,-16], data2_train_temp[,16], k=kk);
  te_knn <- mean( ypred2.test != data2_test_temp[,16])
  
  
  ## 5.1 Desicion Tree
  y1hatc <- ifelse(predict(rpart.chd,data2_train)[,2] < .5, 0, 1)
  tr_tree = sum(y1hatc != ytrain)/length(ytrain)
  y2hatc <-  predict(rpart.chd, data2_test[,-16],type="class")
  te_tree = sum(y2hatc != ytest)/length(ytest)
  
  ## 5.2 Random Forest
  
  mod_rf <- randomForest(as.factor(data2_train$TenYearCHD) ~., data=data2_train, 
                importance=TRUE, ntree = 300, mtry = 5, nodesize=6)
  ## Prediction on the testing data set
  rf_pred_tr2 = predict(mod_rf, data2_train, type='class')
  rf_train_error2 = sum(rf_pred_tr2 != ytrain)/length(ytrain)
  
  ## Prediction on the testing data set
  rf_pred_te2 = predict(mod_rf, data2_test, type='class')
  rf_test_error2 = sum(rf_pred_te2 != ytest)/length(ytest)
  
  
  TEALL[b,] = c(tr_lda,te_lda, tr_qda,te_qda, tr_bayes,te_bayes, tr_logistic,te_logistic, tr_knn, te_knn, tr_tree, te_tree, rf_train_error2, rf_test_error2)
  ###

}




### END ###
```
```{r}
  ## 5.2 Random Forest
TERF <- matrix(NA, nrow = B, ncol = 2)

for (b in 1:B){  
  mod_rf <- randomForest(as.factor(data2_train$TenYearCHD) ~., data=data2_train, 
                importance=TRUE, ntree = 300, mtry = 2, nodesize=2)
  ## Prediction on the testing data set
  rf_pred_tr2 = predict(mod_rf, data2_train, type='class')
  rf_train_error2 = sum(rf_pred_tr2 != ytrain)/length(ytrain)
  
  ## Prediction on the testing data set
  rf_pred_te2 = predict(mod_rf, data2_test, type='class')
  rf_test_error2 = sum(rf_pred_te2 != ytest)/length(ytest)
  TEALL[b,] = c(rf_train_error2, rf_test_error2)
}
```


```{r}
dim(TEALL); ### This should be a Bx9 matrices

mean <- apply(TEALL, 2, mean)
median<- apply(TEALL, 2, median)
var <- apply(TEALL, 2, var)
CI <- 1.96 * (mean/sqrt(var))
frame <- cbind(mean, median, var, CI)
```

```

```{r}
xtable(frame)
```

## GAM 
library(gam)
modgam <- gam( V58 ~ . + + s(V5) + s(V6) + s(V7) + s(V8) + s(V16) + s(V17) + s(V52) + s(V53) + s(V56) + s(V57), 
                   family = binomial(link = "logit"), data= data2_train, trace=TRUE)
phatgam <-  predict(modgam, data2_test[,-58],type="response")
yhatgam <- ifelse(phatgam <0.5,0,1)
sum(yhatgam != ytest)/length(ytest)
# 0.06510417